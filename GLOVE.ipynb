{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tcm(tokenizer, vocabulary, corpus, skip_grams_window):\n",
    "    \"\"\"\n",
    "    Term co-occuarnce matrix.\n",
    "    It would've been dtm.dtmT if there wasn't a window size\n",
    "    :param tokenizer:\n",
    "    :param vocabulary:\n",
    "    :param corpus:\n",
    "    :param skip_grams_window:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    vocab_size = len(vocabulary)\n",
    "    tcm = np.zeros((vocab_size, vocab_size))\n",
    "    for words in data.corpus_to_token_ids(corpus, tokenizer, vocabulary):\n",
    "        L = len(words)\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            left_start, left_end = max(0, i - skip_grams_window), i\n",
    "            right_start, right_end = i + 1, min(i + 1 + skip_grams_window, L)\n",
    "\n",
    "            context_left = words[left_start:left_end]\n",
    "            context_right = words[right_start:right_end]\n",
    "\n",
    "            for j, cword in enumerate(context_left[::-1]):\n",
    "                tcm[word][cword] += 1. / (1 + j)\n",
    "            for j, cword in enumerate(context_right):\n",
    "                tcm[word][cword] += 1. / (1 + j)\n",
    "\n",
    "    np.fill_diagonal(tcm, 0)\n",
    "\n",
    "    return tcm\n",
    "\n",
    "\n",
    "# Implementation of these two functions is borrowed from\n",
    "# https://github.com/erwtokritos/keras-glove/blob/43ce3a262a517e2c7aed04f1726bc7ea049fd031/app/models.py\n",
    "# with modifications and debugging\n",
    "\n",
    "def create_loss(a=0.75, x_max=100):\n",
    "    @tf.function\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        This is GloVe's loss function\n",
    "        :param y_true: The actual values, in our case the 'observed' X_ij co-occurrence values\n",
    "        :param y_pred: The predicted (log-)co-occurrences from the model\n",
    "        :return: The loss associated with this batch\n",
    "        \"\"\"\n",
    "        return K.sum(K.pow(K.clip(y_true / x_max, 0.0, 1.0), a) * K.square(y_pred - K.log(1 + y_true)), axis=-1)\n",
    "\n",
    "    return custom_loss\n",
    "\n",
    "\n",
    "def glove_model(vocab_size, vector_dim):\n",
    "    \"\"\"\n",
    "    A Keras implementation of the GloVe architecture\n",
    "    :param vocab_size: The number of distinct words\n",
    "    :param vector_dim: The vector dimension of each word\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    input_target = tf.keras.layers.Input((1,), name='central_word_id')\n",
    "    input_context = tf.keras.layers.Input((1,), name='context_word_id')\n",
    "\n",
    "    central_embedding = tf.keras.layers.Embedding(vocab_size, vector_dim, input_length=1, name='central_embeddings')\n",
    "    central_bias = tf.keras.layers.Embedding(vocab_size, 1, input_length=1, name='central_biases')\n",
    "\n",
    "    context_embedding = tf.keras.layers.Embedding(vocab_size, vector_dim, input_length=1, name='context_embeddings')\n",
    "    context_bias = tf.keras.layers.Embedding(vocab_size, 1, input_length=1, name='context_biases')\n",
    "\n",
    "    vector_target = central_embedding(input_target)\n",
    "    vector_context = context_embedding(input_context)\n",
    "\n",
    "    bias_target = central_bias(input_target)\n",
    "    bias_context = context_bias(input_context)\n",
    "\n",
    "    dot_product = tf.keras.layers.Dot(axes=-1)([vector_target, vector_context])\n",
    "    dot_product = tf.keras.layers.Reshape((1,))(dot_product)\n",
    "    bias_target = tf.keras.layers.Reshape((1,))(bias_target)\n",
    "    bias_context = tf.keras.layers.Reshape((1,))(bias_context)\n",
    "\n",
    "    prediction = tf.keras.layers.Add()([dot_product, bias_target, bias_context])\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[input_target, input_context], outputs=prediction)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def data_set_generator_from_tcm(tcm):\n",
    "    def _glove_gen():\n",
    "        with np.nditer(tcm, flags=['multi_index']) as it:\n",
    "            for item in it:\n",
    "                first_id, second_id = it.multi_index\n",
    "                if first_id != second_id:\n",
    "                    yield {'central_word_id': [first_id], 'context_word_id': [second_id]}, item\n",
    "\n",
    "    return _glove_gen\n",
    "\n",
    "\n",
    "def get_vectors_from_model(model):\n",
    "    # wv_context = glove$components\n",
    "    # word_vectors = wv_main + t(wv_context)\n",
    "\n",
    "    wv_context = wv_main = None\n",
    "    for layer in model.layers:\n",
    "        if layer.name == 'context_embeddings':\n",
    "            wv_context = layer.weights[0]\n",
    "        elif layer.name == 'central_embeddings':\n",
    "            wv_main = layer.weights[0]\n",
    "\n",
    "    assert wv_main is not None and wv_main is not None\n",
    "\n",
    "    return wv_context + wv_main\n",
    "\n",
    "\n",
    "def create_training_dataset(batch_size, tcm):\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        data_set_generator_from_tcm(tcm),\n",
    "        output_types=({\n",
    "                          'central_word_id': tf.int32, 'context_word_id': tf.int32\n",
    "                      }, tf.float64),\n",
    "        output_shapes=({\n",
    "                           'central_word_id': tf.TensorShape([1, ]), 'context_word_id': tf.TensorShape([1, ])\n",
    "                       }, tf.TensorShape([]))\n",
    "    )\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embeddings(*,\n",
    "                     tcm: np.array,\n",
    "                     vector_dims: int,\n",
    "                     batch_size: int = 2048,\n",
    "                     plot_model: bool = False,\n",
    "                     model_summary: bool = False,\n",
    "                     x_max: int = 50,\n",
    "                     **kwargs):\n",
    "    optimizer = kwargs.pop('optimizer', tf.keras.optimizers.Adam())\n",
    "    save_model_name = kwargs.pop('save_model', None)\n",
    "    save_embedding_name = kwargs.pop('save_embedding', None)\n",
    "\n",
    "    vocab_size = tcm.shape[0]\n",
    "\n",
    "    dataset = create_training_dataset(batch_size, tcm)\n",
    "\n",
    "    model = glove_model(\n",
    "        vocab_size=vocab_size,\n",
    "        vector_dim=vector_dims\n",
    "    )\n",
    "    model.compile(loss=create_loss(x_max=x_max), optimizer=optimizer)\n",
    "    if model_summary:\n",
    "        model.summary()\n",
    "    if plot_model:\n",
    "        tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "\n",
    "    model.fit(dataset, **kwargs)\n",
    "    if save_model_name is not None:\n",
    "        model.save(save_model_name)\n",
    "\n",
    "    embeddings = get_vectors_from_model(model)\n",
    "\n",
    "    if save_embedding_name is not None:\n",
    "        np.save(save_embedding_name, embeddings)\n",
    "\n",
    "    return model, embeddings\n",
    "\n",
    "\n",
    "def load_pretrained_embeddings(**kwargs):\n",
    "    model_address = kwargs.pop('model_address', None)\n",
    "    embedding_address = kwargs.pop('embedding_address', None)\n",
    "\n",
    "    # Split xor into two expressions\n",
    "    assert not (model_address is None and embedding_address is None)\n",
    "    assert model_address is None or embedding_address is None\n",
    "\n",
    "    if embedding_address is not None:\n",
    "        return np.load(embedding_address)\n",
    "\n",
    "    if model_address is not None:\n",
    "        return tf.keras.models.load_model(model_address)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
